{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation CrewAI agent\n",
    "---\n",
    "\n",
    "This notebook presents a solution to build an agent using [CrewAI](https://docs.crewai.com/introduction). CrewAI enables you to create AI teams where each agent has specific roles, tools, and goals, working together to accomplish complex tasks. \n",
    "\n",
    "As a part of this solution, we will build a simple code generation agent that can use some content within a knowledge base and self reflection to provide executable and correct code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install crew ai. For installation steps, follow the instructions here: https://docs.crewai.com/installation\n",
    "!pip install 'crewai[tools]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import boto3\n",
    "import random\n",
    "import logging\n",
    "from globals import *\n",
    "from pathlib import Path\n",
    "from litellm import completion\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Dict, List, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(Path(CONFIG_FILE).read_text())\n",
    "logger.info(f\"config=\\n{json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the current AWS region\n",
    "region = config['general']['region']\n",
    "# the region to be dynamically fetched\n",
    "logger.info(f\"Current AWS region: {region}\")\n",
    "bedrock_agent = boto3.client(service_name = \"bedrock-agent\", region_name = region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name: str = config['general']['role_name']\n",
    "account: str = boto3.client('sts').get_caller_identity()['Account']\n",
    "role = f\"arn:aws:iam::{account}:role/{role_name}\"\n",
    "logger.info(f\"IAM role being used: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the code generation task\n",
    "---\n",
    "\n",
    "In this portion of the solution we create a `gen_code` function that generates Python code for a given problem statement. This function retrieves the prompt template via a Bedrock agent, hydrates the template, and runs inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the model refuses to generate content; we define a custom exception.\n",
    "class NoContentGeneratedException(Exception):\n",
    "    pass\n",
    "\n",
    "# A canned failure response (if needed)\n",
    "FAILED_RESPONSE = \"\"\"\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    input = sys.stdin.read\n",
    "    data = input().split()\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Regular expression to extract Python code blocks wrapped in ```python ... ```\n",
    "REGEX_FOR_PY_CODE_EXTRACTION: str = r\"```python\\n(.*?)```\"\n",
    "\n",
    "def _process_task(model_name: str, formatted_prompt: str, inference_params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Runs inference for a prompt using the specified model.\n",
    "    Retries (with delays and jitter) in case of errors.\n",
    "    \"\"\"\n",
    "    max_retries: int = 10\n",
    "    retry_delay: int = 60  # seconds\n",
    "    logger.info(f\"Running inference with prompt:\\n{formatted_prompt}\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = completion(\n",
    "                model=model_name,\n",
    "                model_id=None,\n",
    "                messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "                max_tokens=inference_params[\"max_tokens\"],\n",
    "                temperature=inference_params[\"temperature\"],\n",
    "                n=inference_params[\"n\"],\n",
    "            )\n",
    "            logger.info(f\"Raw Response: {response}\")\n",
    "            \n",
    "            # If the model returned no completion tokens, raise a custom exception.\n",
    "            if response['usage']['completion_tokens'] == 0:\n",
    "                content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "                raise NoContentGeneratedException(f\"Completion tokens is 0, content={content}\")\n",
    "            \n",
    "            return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        except NoContentGeneratedException as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)\n",
    "                logger.error(f\"{e}, attempt {attempt + 1}. Retrying in {this_retry_delay} seconds...\")\n",
    "                time.sleep(this_retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(\"Max retries exceeded for task (NoContentGeneratedException).\")\n",
    "                raise\n",
    "                \n",
    "        except RateLimitError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                this_retry_delay = retry_delay * (attempt + 1) + random.randint(1, 10)\n",
    "                logger.error(f\"{e}, attempt {attempt + 1}. Retrying in {this_retry_delay} seconds...\")\n",
    "                time.sleep(this_retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(\"Max retries exceeded for task (RateLimitError).\")\n",
    "                raise\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing task: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def hydrate_prompt(prompt_template: str, values: dict) -> str:\n",
    "    \"\"\"\n",
    "    Renders a prompt template using Jinja2.\n",
    "    \"\"\"\n",
    "    from jinja2 import Template\n",
    "    template = Template(prompt_template)\n",
    "    return template.render(values)\n",
    "\n",
    "def gen_code(query: str, model_id: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Uses Amazon Bedrock (via litellm) to generate Python code for a given problem statement.\n",
    "    This function retrieves the prompt template via a Bedrock agent, hydrates the template,\n",
    "    and runs inference.\n",
    "    \"\"\"\n",
    "    # Get the prompt mapping from an environment variable\n",
    "    mapping_str = os.environ.get('MODEL_ID_TO_PROMPT_ID_MAPPING')\n",
    "    logger.info(f\"MODEL_ID_TO_PROMPT_ID_MAPPING: {mapping_str}\")\n",
    "    if mapping_str is not None:\n",
    "        model_id_to_prompt_id_mapping = json.loads(mapping_str)\n",
    "    else:\n",
    "        logger.error(\"MODEL_ID_TO_PROMPT_ID_MAPPING not set in environment.\")\n",
    "        return None\n",
    "    \n",
    "    prompt_id = model_id_to_prompt_id_mapping.get(model_id)\n",
    "    logger.info(f\"Found prompt_id={prompt_id} for model_id={model_id}\")\n",
    "    if prompt_id is not None:\n",
    "        bedrock_agent = boto3.client(service_name=\"bedrock-agent\", region_name=os.environ.get('AWS_REGION', REGION))\n",
    "        prompt_info = bedrock_agent.get_prompt(promptIdentifier=prompt_id)\n",
    "        prompt_template = prompt_info['variants'][0]['templateConfiguration']['text']['text']\n",
    "        prompt = hydrate_prompt(prompt_template, {\"question\": query})\n",
    "        \n",
    "        inference_params = prompt_info['variants'][0]['inferenceConfiguration']['text']\n",
    "        # Adjust key names for litellm: change maxTokens -> max_tokens\n",
    "        inference_params[\"max_tokens\"] = inference_params.pop(\"maxTokens\")\n",
    "        inference_params[\"n\"] = 1\n",
    "        logger.info(f\"Running inference for model_id={model_id} with parameters: {inference_params}\")\n",
    "    else:\n",
    "        logger.error(f\"No prompt id found for model_id={model_id}\")\n",
    "        return None\n",
    "    \n",
    "    bedrock_model_id = f\"bedrock/{model_id}\"\n",
    "    generated_text = _process_task(bedrock_model_id, prompt, inference_params)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tool and assign it as a task\n",
    "---\n",
    "\n",
    "Now, we will create a code generation tool that will wrap the code generation functionality. This tool will be registered to the crewAI agent and invoked once a user asks a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it is us-east-1 or us-west-2, use the inference profile model id\n",
    "if \"us-east-1\" in region or \"us-west-2\" in region:\n",
    "    CODE_GEN_MODEL_ID = f\"us.{CODE_GEN_MODEL_ID}\"\n",
    "    logger.info(f\"Using inference profile model id: {CODE_GEN_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from crewai.tools import tool\n",
    "\n",
    "@tool(\"Code Generation Tool\")\n",
    "def code_generation_tool(question: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool generates Python code for a given USACO problem statement.\n",
    "    \n",
    "    It returns a solution that is wrapped in markdown code block delimiters.\n",
    "    The agent will extract the code block for further processing.\n",
    "    \"\"\"\n",
    "    # Dummy code-generation logic for demonstration.\n",
    "    generated_text = gen_code(question, model_id=CODE_GEN_MODEL_ID)\n",
    "    if generated_text is None:\n",
    "        return \"Error: Code generation failed.\"\n",
    "    # Attempt to extract a Python code block wrapped by ```python ... ```\n",
    "    regex = r\"```python\\n(.*?)```\"\n",
    "    match = re.search(regex, generated_text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return generated_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CrewAI agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import LLM\n",
    "# customize your own LLM\n",
    "amazon_nova_llm = LLM(\n",
    "    model=CODE_GEN_MODEL_ID,  \n",
    "    temperature=0.1,\n",
    "    max_tokens=256,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Create a Code Generation Agent that registers the CodeGenerationTool.\n",
    "code_gen_agent = Agent(\n",
    "    role=\"Code Generation Agent\",\n",
    "    goal=\"Generate Python code for USACO problems.\",\n",
    "    backstory=\"An expert in code generation using Amazon Bedrock, focused on USACO challenges.\",\n",
    "    tools=[code_generation_tool],\n",
    "    verbose=True, \n",
    "    llm=amazon_nova_llm,\n",
    ")\n",
    "\n",
    "# Define a task for code generation.\n",
    "code_task = Task(\n",
    "    description=\"Generate Python code for the following USACO problem: {question}\",\n",
    "    expected_output=\"Python code solution for the provided USACO problem.\",\n",
    "    agent=code_gen_agent,\n",
    "    output_file=\"generated_code.py\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew that includes the Code Generation Agent.\n",
    "crew = Crew(\n",
    "    agents=[code_gen_agent],\n",
    "    tasks=[code_task],\n",
    "    verbose=True,\n",
    "    planning=True,\n",
    "    planning_llm=amazon_nova_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = (\"What is the code to upload files to Amazon S3?\").strip()\n",
    "crew_inputs = {\"question\": problem_statement}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off the Crew. This will delegate the task to the Code Generation Agent,\n",
    "# which in turn will use the CodeGenerationTool to generate the Python code.\n",
    "crew.kickoff(inputs=crew_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
